require 'optim'
require 'xlua' -- for progress bars and other graphics


-- Enable this flag if you have a cuda capable gpu and have cunn installed.
cuda = true


-- loading the data
dofile 'dataGen.lua'
data = Data()

-- loading the model
model = dofile 'model.lua'
-- If you have a cuda capable gpu,

--[[A model's getParameters method returns
1. parameters (A 1D tensor of all the weights of all the modules)
2. gradParameters Aa 1D tensor, the same size as that of parameters, which is updated with gradients by model:backward(), this is equivalent to updates if you are familiar with theano.)

Naturally, these are the playground of optim
]]--
  parameters,gradParameters = model:getParameters()

-- Used to describe the performance of a classifier
  confusion = optim.ConfusionMatrix(43)

-- this is a lua table, passed to optim for configuring the optimizer.
optimState = {
      learningRate = 0.01,
      learningRateDecay = 0.0,
      momentum = 0.9,
      nesterov = true,
      dampening = 0.0,
      weightDecay = 1e-6,
   }

-- The cross entropy loss function, criterion.output stores the loss from the latest criterion:forward() call.

    criterion = nn.CrossEntropyCriterion()


if cuda == true then
  require 'cunn'
  require 'cudnn'
  criterion = criterion:cuda()
  model = model:cuda()
  cudnn.convert(model,cudnn)
end

print(model)

--[[
This function trains the model for one epoch. The loop iterates over the train data in batches, performs forward on the model,
calculates the loss and the gradients at output layer using the criterion and finally calls backward on the model.
The updates generated by the backward call are stored in gradParameters which are then used by optim to update the parameters.
]]--

function train(epoch)
    epoch = epoch or 1  -- if epoch is not passed, initializes to 1
--  for modules like dropout, which behave differently during train and test.
    model:training()
--  learning rate schedule, half the learning rate every 50 epochs
    if epoch % 50 == 0 then
        optimState.learningRate = optimState.learningRate/2
    end

--[[
function which returns the loss and the gradParameters(updates to be made to parameters),    internally called by optim.
--]]
    local function feval(x)
	if x ~= parameters then parameters:copy(x) end
        return criterion.output, gradParameters
    end
    local training_loss = 0
    local trSize = data:getTrainDataSize()
    local trIndex = 0
    for inputs,targets in data:TrainGenerator(32) do
        trIndex = trIndex + inputs:size(1)
        xlua.progress(trIndex,trSize)

        if cuda == true then
          inputs = inputs:cuda()
          targets = targets:cuda()
        end

--      we don't want to keep updates from the prev batch.
        gradParameters:zero()
        local outputs = model:forward(inputs)
        local loss = criterion:forward(outputs, targets)
        local dloss_dx = criterion:backward(outputs, targets)
        training_loss = training_loss + loss*inputs:size(1)
	      model:backward(inputs, dloss_dx)
        confusion:batchAdd(outputs, targets)
        optim.sgd(feval,parameters,optimState)
        collectgarbage()
    end
--  Book keeping
    confusion:updateValids()
    train_acc = confusion.totalValid * 100
    print('Train Loss: '..training_loss/trSize)
    print(('Train accuracy: %.2f'):format(train_acc))
    confusion:zero()
    epoch = epoch + 1
end

-- The test function is extremely similar except that we don't need to update the parameters

function test()
--  disable dropouts and batch normalization
    model:evaluate()
    local teSize = data:getTestDataSize()
    local teIndex = 0
    for inputs,targets in data:TestGenerator(128) do
      teIndex = teIndex + inputs:size(1)
      xlua.progress(teIndex,teSize)
      if cuda == true then
        inputs = inputs:cuda()
        targets = targets:cuda()
      end

      local outputs = model:forward(inputs)
      confusion:batchAdd(outputs, targets)
    end
    confusion:updateValids()

    local test_acc = confusion.totalValid * 100
    confusion:zero()
    print(('Test accuracy: %.2f'):format(test_acc))
end

-- Now that everything is set, let us train our network for a 100 epochs
max_epoch = 100
for i = 1,100 do
    train()
    test()
end

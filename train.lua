require 'optim'
require 'xlua' -- for progress bars and other graphics


-- Enable this flag if you have a cuda capable gpu and have cunn installed.
cuda = true


-- Loading the data
require 'dataGen.lua'
dgen = DataGen('GTSRB/')

-- Loading the model
model = dofile 'model.lua'
-- If you have a cuda capable gpu,

--[[A model's getParameters method returns
1. parameters : A 1D tensor of all the weights of all the modules)
2. gradParameters: A 1D tensor, the same size as that of parameters. It is updated with gradients 
   by model:backward().

Naturally, these are the playground of optim
]]--
parameters,gradParameters = model:getParameters()


-- Used to measure the performance of a classifier
confusion = optim.ConfusionMatrix(43)

-- This is a lua table, passed to optim for configuring the optimizer.
optimState = {
      learningRate = 0.01,
      learningRateDecay = 0.0,
      momentum = 0.9,
      nesterov = true,
      dampening = 0.0,
      weightDecay = 1e-6,
   }
batch_size = 32

-- The cross entropy loss function, criterion.output stores the loss from the latest criterion:forward() call.
criterion = nn.CrossEntropyCriterion()


-- Tranfer model and criterion to cuda if you have have it
if cuda == true then
  require 'cunn'
  require 'cudnn'
  criterion = criterion:cuda()
  model = model:cuda()
  cudnn.convert(model,cudnn)
end

print(model)

--[[
This function trains the model for one epoch. The loop iterates over the train data in batches and
  1. performs forward on the model,
  2. calculates the loss and the gradients at output layer using the criterion,
  3. finally calls backward on the model.
  4. The updates generated by the backward call update gradParameters which are then used by optim to 
  update the parameters.
]]--

function train(epoch)
  epoch = epoch or 1  -- if epoch is not passed, initializes to 1
  
  -- Modules like dropout behave differently during train and test. So, switch to training mode.
  model:training()
  
  -- Learning rate schedule, half the learning rate every 15 epochs
  if epoch % 15 == 0 then
      optimState.learningRate = optimState.learningRate/2
  end

  local training_loss = 0
  local trSize = dgen.nbTrainExamples
  local trIndex = 0
  
  -- Loop over the dataset. 
  for inputs,targets in dgen:trainGenerator(batch_size) do
      trIndex = trIndex + inputs:size(1)
      xlua.progress(trIndex,trSize)

      if cuda == true then
        inputs = inputs:cuda()
        targets = targets:cuda()
      end

      --[[
      Function which returns the loss and the gradParameters(updates to be made to parameters).
      Internally called by optim.
      --]]
 
      local function feval(x)
        if x ~= parameters then parameters:copy(x) end

        -- We don't want to keep updates from the prev batch. So clear it.
        gradParameters:zero()
      
        -- core of the training
        local outputs = model:forward(inputs)
        local loss = criterion:forward(outputs, targets)
        local dloss_dx = criterion:backward(outputs, targets)
        model:backward(inputs, dloss_dx)

        confusion:batchAdd(outputs, targets)
      end

      _,fs = optim.sgd(feval,parameters,optimState)
      training_loss = training_loss + fs[#fs] * inputs:size(1)
      
      collectgarbage()
  end

  -- Book keeping
  confusion:updateValids()
  train_acc = confusion.totalValid * 100
  print('Train Loss: '..training_loss/trSize)
  print(('Train accuracy: %.2f'):format(train_acc))
  confusion:zero()
  epoch = epoch + 1
end

-- The test function is extremely similar except that we don't need to update the parameters

function test()
    -- Modules like dropout behave differently during train and test. So, switch to evaluate mode.
    model:evaluate()
    
    local teSize = dgen.nbValExamples
    local teIndex = 0
    
    for inputs,targets in dgen:valGenerator(128) do
      teIndex = teIndex + inputs:size(1)
      xlua.progress(teIndex,teSize)
      if cuda == true then
        inputs = inputs:cuda()
        targets = targets:cuda()
      end

      local outputs = model:forward(inputs)
      confusion:batchAdd(outputs, targets)
    end
    confusion:updateValids()

    local test_acc = confusion.totalValid * 100
    confusion:zero()
    print(('Test accuracy: %.2f'):format(test_acc))
end

-- Now that everything is set, let us train our network for a 100 epochs
max_epoch = 100
for i = 1,100 do
    train()
    test()
end
